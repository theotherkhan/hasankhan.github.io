<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Hasan Khan</title>
    <link>https://theotherkhan.github.io/projects/</link>
    <description>Recent content on Hasan Khan</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>© Hasan Khan</copyright>
    <lastBuildDate>Tue, 23 May 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://theotherkhan.github.io/projects/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Curriculum Learning for Knowledge Distillation in LMs</title>
      <link>https://theotherkhan.github.io/projects/kdclsrlm/</link>
      <pubDate>Tue, 23 May 2023 00:00:00 +0000</pubDate>
      
      <guid>https://theotherkhan.github.io/projects/kdclsrlm/</guid>
      <description>Abstract Knowledge distillation (KD) is a powerful, well-established model compression technique that can face performance limitations when the capacity difference between student and teacher models is severely mismatched (Cho and Hariharan, 2019), or when multiple teachers cause competing distillation objectives (Du et al., 2020). To address these issues and improve performance in KD for large language modeling, I explore the implementation of two ideas: using curriculum learning during KD, where training data is sorted based on difficulty, and using Selective Reliance during KD, where a student language model selectively leverages teacher distillation loss for data samples deemed difficult by the curriculum. See the full report here.
Introduction Knowledge Distillation (Hinton et al., 2015) is a model compression technique commonly deployed in settings where large models are difficult to store and run. Vanilla KD comprises of a dual model student-teacher modeling framework, where a small capacity student model aims to mimic the performance of a larger capacity teacher model by learning the distribution of the output labels generated from the teacher model trained on the same dataset. Specifically, the student model uses a bipartite loss function L_student that incorporates both L_kd (the KD loss measured by the KL divergence between the softmax of the student output logits PS and the softmax of the teacher output logits P_T , scaled by the temperature parameter τ ) and L_ce (the standard cross entropy training loss using the true labels ytrue). The parameter λ controls the weight given to each component loss. The student loss and its component losses are defined below:
$$ L_{ce} = CE(y_{true},P_S) $$ $$ L_{kd} = \tau^2 KL(P_T,P_S) $$ $$ L_{student} = (1 - \lambda)L_{ce} + \lambda L_{kd} $$
The general framework is visualized below:
However, KD has been shown to provide minimal to no performance gains for certain tasks, such as image recognition on ImageNet (Zagoruyko and Komodakis, 2016). Cho and Hariharan note that empirical results establishing the generality of KD efficacy for various tasks are nonexistent. They examine the reasons behind these failures, noting that large differences in student teacher model capacities may limit the student model’s ability to minimize both training loss and KD loss, forcing the student to minimize the KD loss over the train loss. In addition to capacity gaps between student and teachers, KD frameworks that involve distillation from an ensemble of teachers require n-paritite loss functions, in which conflict can exists between different teacher models, which can adversely effect distillation loss (Du et al., 2020).
From these findings, I propose selective reliance, a KD technique for dynamically changing the student’s reliance on KD loss, in order to improve model accuracy. Selective reliance is implemented by updating λ in LStudent at the sample level, where λ is determined by the difficulty of the sample. Difficulty is defined in the context of Curriculum Learning (CL), a training strategy for improving model convergence speed and accuracy that involves training on easily learnable samples before more difficult ones (Bengio et al., 2009).
Difficulty rankings for samples can be determined using the confidence scores generated by either the teacher model (teacher-generated curriculum) or the student model (student-generated curriculum). In the former, the teacher model would also act as the difficulty scoring function (Hacohen and Weinshall, 2019) for each datapoint in the the student’s curriculum, calculated before the student training process. In the latter, the student model would generate its own curriculum during student training by looking at confidence scores generated during previous epochs in training, as described by snapshot learning (Zhao et al., 2021). For this experiment, I use the confidence scores generated by the student model (student-generated curriculum) during training. I hypothesize that relying on the teacher model is only in the student’s best interest when the sample being evaluated is &amp;ldquo;difficult&amp;rdquo; as determined by the curriculum, and that accuracy yielded from vanilla knowledge distillation (KD) and knowledge distillation with curriculum learning (KD-CL) can be improved upon with selective reliance (KD-CL-SR) techniques built into the distillation framework.
See the full report here.</description>
    </item>
    
    <item>
      <title>Neural Question Generation with GPT-J</title>
      <link>https://theotherkhan.github.io/projects/nqj/</link>
      <pubDate>Sat, 23 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://theotherkhan.github.io/projects/nqj/</guid>
      <description>Abstract Neural Question Generation (QG) systems aim to automate the process of question construction by generating novel questions given a particular context, thus reducing time and costs of question generation for educators and test developers. We propose Question Generation using GPT-J in a few-shot setting. Generating questions in this manner reduces time and resource cost required to construct datasets and fine-tune increasingly complex models like GPT-J, thereby increasing usage for educational purposes such as adaptive education. We compare our results against a GPT-J model fine-tuned on the task. See the full paper here.
Introduction Asking relevant questions of varying difficulty forms an essential part of assessing a student’s grasp of concepts. Instructors are required to spend considerable amounts of time constructing exam and assignment questions that assess students on material taught in the classroom. In this process of question creation, instructors must make sure that the questions cannot all be of similar difficulty and must range from easy to difficult to ensure optimum learning outcomes. Additionally, questions often need to be replaced continually as content is revised to reflect the latest updates in the domain or because the questions become publicly available. Another aspect that affects learning outcomes is the student&amp;rsquo;s aptitude and pre-existing knowledge. In classroom settings, learning takes place in groups in which some students would be stronger in the concepts the material tests while others would not and would require additional practice and/or instruction. As a result, applying universal pedagogy in such settings is not ideal (Lieu et al., 2020). Adaptive education systems have great potential to improve learning outcomes by increasing accessibility (Srivastava and Goodman, 2021).
Neural Question Generation (QG) (Pan et al., 2019) systems aim to automate the process of question construction by generating novel questions given a particular context, thus reducing time and costs of question generation for educators and test developers. Advanced QG systems with configurable parameters could help offer students custom material based on their individual ability, and act as a foundation for adaptive testing and learning.
Recent work in QG has focused on generating quiz-style questions (Lelkes et al., 2021), with particular focus on generating questions of selected difficulty levels (Gao et al., 2018). However, these techniques have relied on fine-tuning a language model on a task-specific dataset such as SQuAD or RACE (Lai et al., 2017). As a result, these models are limited in their domain of use. Moreover, constructing such datasets that contain thousands of examples, specific to that task, is time-consuming and costly, and thus not a viable means for widespread adoption. In this paper, we propose using GPT-J in a few-shot setting to produce questions that are fluent in linguistic construction, relevant to the input context, and appropriately difficult as desired. We compare our work against a GPT-J model fine-tuned on the task.
In the context of reading comprehension, we look at two types of QG variants. In answer-focused QG, a reference passage and an answer are passed as inputs into the system, resulting in the generation of questions relevant to the input answer. In general QG, only a context passage is passed as input, resulting in the generation of unmapped questions relevant to the context.
See the full paper here.</description>
    </item>
    
    <item>
      <title>Selective Teacher Reliance for Knowledge Distillation</title>
      <link>https://theotherkhan.github.io/projects/kdclsr/</link>
      <pubDate>Thu, 21 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://theotherkhan.github.io/projects/kdclsr/</guid>
      <description>Abstract Knowledge distillation (KD) is a powerful, well established model compression technique that can face performance limitations when student models attempt to mimic large teacher models on high dimensional tasks like image classification (Cho and Hariharan, 2019). Motivated from ideas in curriculum learning, we explore the idea of selective reliance on the task of image recognition, where a student model relies more heavily on teacher guidance for data samples deemed difficult by a teacher generated curriculum. Experimental results show minimal effect of curriculum setting and selective reliance techniques on student accuracy and convergence. See the full report here.
Introduction Knowledge Distillation (Hinton et al, 2015) is a model compression technique commonly used in applied settings where large models are difficult to store and run. KD comprises of a dual model student-teacher framework, where a small capacity student model aims to mimic the performance of a larger capacity teacher model by learning the distribution of the output labels generated from the teacher model trained on the same dataset. More specifically, the student model uses a bipartite loss function L_student that incorporates both L_kd (the KD loss measured by the KL divergence between the softmax of the student output logits P_S and the softmax of the teacher output logits P_T, scaled by the temperature parameter τ) and L_ce (the standard cross entropy training loss using the true labels y_true). The parameter λ controls the weight given to each component loss. The student loss and its component losses are defined below:
$$ L_{ce} = CE(y_{true},P_S) $$ $$ L_{kd} = \tau^2 KL(P_T,P_S) $$ $$ L_{student} = (1 - \lambda)L_{ce} + \lambda L_{kd} $$
However, KD has been shown to provide minimal to no performance gains for certain tasks such as image recognition on ImageNet (Zagoruyko and Komodakis, 2016). Cho and Hariharan (2019) examine the reasons behind failures in this context, noting that large differences in student teacher model capacities may limit the student model&amp;rsquo;s ability to minimize both training loss and KD loss, forcing the student to minimize the KD loss over the train loss.
From this finding, we propose selective reliance, a technique for dynamically changing the student&amp;rsquo;s reliance on KD loss by updating λ in L_{Student at the sample level, where λ is determined by the difficulty of the sample. Difficulty is defined in the context of Curriculum Learning (CL), a training strategy for improving model convergence speed and accuracy that involves training on easily learnable samples before difficult ones.
Difficulty rankings for samples can be determined using the confidence scores of additional models, as outlined by Weinshall and Cohen (2018). In the context of knowledge distillation, we use the teacher model to be distilled as the scoring function (Haconen and Weinshall, 2018) for each datapoint in the the student&amp;rsquo;s curriculum. We build closely on work conducted by Zhao et al (2021) where curriculum learning is also used to improve knowledge distillation on image recognition tasks, but aim to employ a distinct curriculum generation scheme from theirs, and a novel mechanism for its effect on KD loss utilization. We hypothesize that relying on the teacher is only in the student&amp;rsquo;s best interest when the sample being evaluated is difficult, and that accuracy yielded from KD w/ CL (KD-CL) can be improved upon with selective reliance (KD-CL-SR) techniques built into KD.
See the full report here.</description>
    </item>
    
    <item>
      <title>Detecting Atrial Fibrillation Burden Using 1-D CNNs</title>
      <link>https://theotherkhan.github.io/projects/afib/</link>
      <pubDate>Fri, 10 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>https://theotherkhan.github.io/projects/afib/</guid>
      <description>Abstract 1-D CNNs have recently been used to classify various classes of Arrhythmias using ECG sequences. In this paper, a 1-D CNN is used to predict the Atrial Fibrillation (AF) burden of ECG sequences, which is a useful continuous metric that helps gauge AF severity and can be predictive for risk of near term stroke. See the full paper here.
Introduction Anywhere from 2.7-6.0 million Americans suffer from Atrial Fibrillation, the leading type of heart arrhythmia (Mina, 2020). Atrial Fibrillation is characterized by an irregular heartbeat; if left untreated, it can lead to blood clots, stroke and other serious heart complications. Historically, cardiologists and technicians manually scan Electrocardiogram (ECG) recordings of patients to determine the presence of AF in patients, often with the assistance of peak detection and interval analysis algorithms. However, these algorithms have a high error rate (Shah, 2007), and in recent years, a plethora of statistical models and machine learning techniques such as Convolutional Neural Networks (CNNs) (Rajpurkar, 2017) and Markov Models (Coast, 1990), among others, have been successfully applied to arrhythmia and AF detection problems, often out preforming traditional methods. These models are attractive because they can help save cardiologist time, increase accessibility to AF diagnosis, and are especially effective for continuous monitoring when paired with wearable technologies that can measure ECG signals.
However, these techniques are often focused on the classification and sub typing of ECG sequences. In this project, the objective is modified to calculate the AF Burden of each sequence instead, a continuous variable defined as the proportion of an ECG sequence that displays AF characteristics. AF Burden has been shown to be associated with near term stroke risk (Lin, 2018), and depending on the duration of the ECG, can be used to help classify between variants of AF, such Paroxysmal AF (episodic AF) and Persistent AF (chronic AF).
This project builds on previous AF classification techniques, modifying them slightly for the purposes of predicting AF Burden. ECG signals from the The 4th China Physiological Signal Challenge (Wang, 2021) are used as the input data for the model, from which the project is inspired.
See the full paper here.</description>
    </item>
    
  </channel>
</rss>
